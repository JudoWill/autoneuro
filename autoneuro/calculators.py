# AUTOGENERATED! DO NOT EDIT! File to edit: 00_calculators.ipynb (unless otherwise specified).

__all__ = ['AbstractCalculator', 'AbstractOperation', 'EquationOperation', 'NormativeLookup', 'BinnedScalingOperator',
           'RegressionNormOperator']

# Cell
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sbn
import numexpr as ne



# Cell
class AbstractCalculator(object):
    fields = []
    operations = []
    inferred_cols = []

    def __init__(self, name, operations):
        """

        Parameters
        ----------
        fields : list[str]
        transforms : list[AbstractOperation]
        """

        self.name = name
        self.operations = operations

        fields = sum((op.fields for op in operations), start = [])
        fields = set(fields)

        inferred = sum((op.result_fields for op in operations), start = [])
        inferred = set(inferred)

        self.fields = sorted(fields-inferred)
        self.inferred_cols = sorted(inferred)

    def to_series(self, row):

        series = pd.Series(dict((field, row.get(field)) for field in self.fields))
        return series

    def explain(self, row):

        ins = [f'{f}:{row[f]}' for f in self.fields]
        print('Taking:', ', '.join(ins))

        res = self.process_single(row, explain=True)
        outs = [f'{f}:{res[f]}' for f in self.inferred_cols]
        print('Resulting in:', ', '.join(outs))

    def __add__(self, other):

        return AbstractCalculator(self.name, self.operations+other.operations)

    def process_single(self, row, explain=False):
        """

        Parameters
        ----------
        row : pd.Series,dict
        explain : bool

        Returns
        -------

        """

        data = self.to_series(row)
        if self.operations:
            for operation in self.operations:
                for field, val in operation(data):
                    data[field] = val
                if explain:
                    print(operation.explain(data))

            #print(data)
        return pd.Series(data)


    def process_dataframe(self, df, mapping = None):
        """

        Parameters
        ----------
        df : pd.DataFrame
        mapping : dict
        Returns
        -------
        pd.DataFrame

        """

        if mapping is not None:
            clean_data = df.rename(columns=mapping)
        else:
            clean_data = df

        #print(clean_data[self.fields])

        res = clean_data.apply(self.process_single, axis=1)
        return res


# Cell
class AbstractOperation(object):

    fields = []
    result_fields = []

    @staticmethod
    def from_config(config):
        op_classes = [EquationOperation,
                      AggregationOperation,
                      ClipOperation,
                      NormativeLookup,
                      BinnedScalingOperator,
                      RegressionNormOperator]

        for op_class in op_classes:
            op = op_class.from_config(config)
            if op is not None:
                return op
        raise NotImplementedError(f'Did not understand type: {config["type"]}')
        #return None

    def process_single(self, row):
        raise NotImplementedError

    def explain(self, row):
        raise NotImplementedError


    def to_series(self, row):

        series = pd.Series(dict((field, row.get(field)) for field in self.fields))
        return series

    def __call__(self, row):

        res = self.process_single(row)
        yield self.result_fields[0], res

# Cell

class EquationOperation(AbstractOperation):

    def __init__(self, out_field, equation, fields):

        self.fields = fields
        self.equation = equation
        self.result_fields = [out_field]

    @staticmethod
    def from_config(config):
        if config['type'] == 'equation':
            return EquationOperation(config['out_field'],
                                     config['equation'],
                                     config['fields'])
        return None

    def explain(self, row):
        res = self.process_single(row)
        return f'Used Equation: {self.equation} = {res} = {self.result_fields[0]}'

    def process_single(self, row):

        data = self.to_series(row)
        #print(data)
        if data.notnull().all():
            res = ne.evaluate(self.equation, local_dict=data)
        else:
            res = np.nan
        return res

# Cell

class NormativeLookup(AbstractOperation):

    def __init__(self, lookup_table, filter_cols, measure_col, out_name):

        self.lookup_table = lookup_table
        self.filter_cols = filter_cols
        self.fields = filter_cols + [measure_col]
        self.result_fields = [out_name]
        self.measure_col = measure_col

    @staticmethod
    def from_config(config):

        if config['type'] == 'normative_lookup':

            return NormativeLookup(config['table'],
                                   config['filter_cols'],
                                   config['measure_col'],
                                   config['out_name'])
        return None

    def lookup_norm(self, row):

        data = self.to_series(row)
        for filt in self.lookup_table:
            if ne.evaluate(filt['filter'], local_dict=data):
                return filt['filter'], filt['mean'], filt['std']

        return None, None, None

    def explain(self, row):

        flt, mean, std = self.lookup_norm(row)

        if flt is None:
            data = self.to_series(row)
            return f'Could not find matching filter for {data[self.filter_cols]}'
        else:
            return f'Matched {flt}, Expecting {mean} +- {std}'

    def process_single(self, row):

        data = self.to_series(row)
        _, mean, std = self.lookup_norm(data)
        if mean is None:
            return np.nan
        return (data[self.measure_col] - mean)/std


# Cell

class BinnedScalingOperator(AbstractOperation):
    def __init__(self, bins, measure_col):

        self.fields = [measure_col]
        self.result_fields = [measure_col+'_scaled']
        self.bins = sorted(bins, key = lambda x: x['min'],
                           reverse=True)

    @staticmethod
    def from_config(config):

        if config['type'] == 'binned_scaling':
            return BinnedScalingOperator(config['bins'],
                                         config['measure_col'])
        return None

    def lookup_bin(self, row):

        data = self.to_series(row)
        val = data[self.fields[0]]
        if val == val:
            for bin in self.bins:
                if val >= bin['min']:
                    return bin['min'], bin['scaled']
            return np.nan, np.nan
        else:
            return np.nan, np.nan

    def explain(self, row):

        edge, scaled = self.lookup_bin(row)

        if edge != edge:
            data = self.to_series(row)
            return f'Could not find matching bin for {data[self.fields[0]]}'
        else:
            return f'{self.fields[0]} matched {edge}, scaled to {scaled}'

    def process_single(self, row):
        _, res = self.lookup_bin(row)
        return res

# Cell

class RegressionNormOperator(AbstractOperation):

    def __init__(self, regressions, fields, out_field, result_type = 'zscale'):

        self.regressions = regressions
        self.fields = fields
        self.result_fields = [out_field]
        self.result_type = result_type

    @staticmethod
    def from_config(config):
        if config['type'] == 'regression_norm':
            return RegressionNormOperator(config['regressions'],
                                          config['fields'],
                                          config['out_field'],
                                          result_type = config['result_type'])

        return None

    def search_filters(self, row):

        data = self.to_series(row)
        check_func = lambda reg: pd.eval(reg['filter'], local_dict=data.to_dict())
        return [reg for reg in self.regressions if check_func(reg)]

    def scale_data(self, row):

        data = self.to_series(row)
        hits = self.search_filters(row)
        if hits: #Currently only implementing "first"
            reg = hits[0]
            val = pd.eval(reg['norm'], local_dict=data.to_dict())
            return reg, val
        return None, None


    def explain(self, row):

        data = self.to_series(row)
        reg, val = self.scale_data(row)

        if reg is None:
            return 'Could not find a match for regression normalization.'
        else:
            return f'Matched {reg["filter"]}, applied {reg["norm"]} = {float(val)}'


    def process_single(self, row):

        _, val = self.scale_data(row)
        return val

