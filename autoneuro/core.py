# AUTOGENERATED! DO NOT EDIT! File to edit: 00_core.ipynb (unless otherwise specified).

__all__ = ['LookupNormalTransform', 'FILTER_MAPPINGS', 'build_merged_transformers', 'build_from_yaml',
           'EncodingTransform', 'ScaledRegressionNode', 'ScaledRegressionTransform']

# Cell
#export

import pandas as pd
import numpy as np
import numexpr as ne
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from scipy.stats import norm
import yaml
import os
from itertools import chain



# Cell
#hide

FILTER_MAPPINGS = {'gt': '>',
                   'gte': '>=',
                   'lte': '<=',
                   'lt': '<',
                   'eq': '==',
                   'neq': '!='}



def _build_filters(lookup_table):
    """
    Build filters from lookup table
    Parameters
    ----------
    lookup_table : pd.DataFrame

    Returns
    -------

    """


    filts = dict((_id, []) for _id in lookup_table.index)
    expected = set()
    for col in lookup_table.columns:
        if '__' in col:
            field, nop = col.split('__')
            op = FILTER_MAPPINGS.get(nop, None)
            if op is not None:
                expected.add(field)
                for ind, val in lookup_table[col].to_dict().items():
                    if type(val) == str:
                        filts[ind].append(f'({field} {op} "{val}")')
                    else:
                        filts[ind].append(f'({field} {op} {val})')
    return pd.Series([' & '.join(filts[ind]) for ind in lookup_table.index],
                     index = lookup_table.index), sorted(expected)

def _build_lookup_norms(lookup_table):
    """

    Pulls out the values of the __mean and __std columns to create a
    pd.Series of scipy.norm instances.

    This can be better generalized in the future.

    Parameters
    ----------
    lookup_table : pd.DataFrame

    Returns
    -------
    ps.Series
    """

    mean_col = next(col for col in lookup_table.columns if col.endswith('__mean'))
    std_col = next(col for col in lookup_table.columns if col.endswith('__std'))

    field = mean_col.split('__')[0]

    return mean_col, std_col, field



    norm_list = []
    for _, row in lookup_table.iterrows():
        norm_list.append(norm(loc = row[mean_col],
                               scale = row[std_col]))
    return pd.Series(norm_list, index = lookup_table.index), fields




class LookupNormalTransform(object):

    def __init__(self, lookup_table=None, name = ''):
        """

        Parameters
        ----------
        lookup_table : pd.DataFrame
          Formatted lookup-table for the transformer
        """

        self.lookup_table = lookup_table
        self.name = name
        self._fit(lookup_table)


    def _fit(self, lookup_table):
        self._filters, self._filter_cols = _build_filters(lookup_table)
        self._mean_col, self._std_col, self._measure_col = _build_lookup_norms(lookup_table)

    @property
    def needed_cols(self):
        """

        Returns
        -------
        list[str]

        """
        return self._filter_cols + [self._measure_col]

    def get_feature_names(self):
        """

        Returns
        -------
        list[str]
        """
        return [self.name]

    def match_samples(self, data, keep = 'last'):
        """ Match samples to their respective rows.

        Parameters
        ----------
        data : pd.DataFrame
        keep : str
            Which item to keep. Currently only accepts 'last'

        Returns
        -------
        pd.Series

        """

        linker_func = lambda flt: data.query(flt).index
        all_matches = [(num, linker_func(f)) for num, f in  self._filters.to_dict().items()]

        data2flt = {}
        for num, matches in all_matches:
            for d_ind in matches:
                data2flt[d_ind] = num
        data2flt = pd.Series(data2flt)
        return data2flt.reindex(data.index)


    def normalize_samples(self, data, matched_rows = None):
        """ Normalize samples to z-scale based on the scheme

        Parameters
        ----------
        data : pd.DataFrame
        matched_rows : pd.Series
          Pre-calculated matching of the rows to each filter index.

        Returns
        -------
        pd.Series

        """

        if matched_rows is None: matched_rows = self.match_samples(data)

        valid_matches = matched_rows.dropna()
        obs_vals = data.loc[valid_matches.index, self._measure_col]
        means = self.lookup_table.loc[valid_matches.values, self._mean_col]
        stds = self.lookup_table.loc[valid_matches.values, self._std_col]
        Zs = pd.Series((obs_vals.values-means.values)/stds.values,
                       valid_matches.index,
                       name = self.name)

        return Zs.reindex(data.index)


    def transform(self, data):
        """

        Parameters
        ----------
        data : pd.DataFrame

        Returns
        -------
        pd.DataFrame

        """

        return pd.DataFrame(self.normalize_samples(data))


# Cell

def build_merged_transformers(df, groupings, universal_cols = None,
                              prefix = '',
                              transform_class = LookupNormalTransform,
                              skip_final = False):
    """
    Build a set of merged transformers.

    Parameters
    ----------
    df : pd.DataFrame
    groupings : dict
    universal_cols : list
    prefix : str
    transform_class : cls
    skip_final : bool
      Useful if chaining multiple calls together before constructing the transformer.

    Returns
    -------
    ColumnTransformer

    """

    if universal_cols is None: universal_cols = []

    for key, cols in groupings.items():
        yield transform_class(name = key,
                              lookup_table=df[cols+universal_cols])


# Cell

def build_from_yaml(path, _nested = False):

    if type(path) is str:
        config = yaml.full_load(open(path))
        direc = os.path.dirname(path)
        lk_data_path = os.path.join(direc, config['lookup_table'])
        lk_data = pd.read_csv(lk_data_path)
        return build_merged_transformers(lk_data, config['groupings'],
                                         universal_cols = config['universal_cols'],
                                         prefix = config['short_name'],
                                         transform_class = LookupNormalTransform,
                                         skip_final = _nested)
    else:
        all_transformers = chain.from_iterable(build_from_yaml(p) for p in path)
        return all_transformers

# Cell

class EncodingTransform(object):

    def __init__(self, replacements = None, renames = None):

        if replacements is None: replacements = {}
        if renames is None: renames = {}

        self.replacements = replacements
        self.renames = renames
        self.rev_renames = dict((val, key) for key, val in renames.items())


    def reverse_col_lookup(self, col):
        return self.rev_renames.get(col, col)

    def forward_col_lookup(self, col):
        return self.renames.get(col, col)

    def extract_data(self, data, needed_cols, added_cols = None, replacements = None):
        """

        Parameters
        ----------
        data : pd.DataFrame
        needed_cols: list[str]
        added_cols : dict
        replacements : dict

        Returns
        -------

        pd.DataFrame

        """

        if replacements is None: replacements  = {}
        needed_cols = set(self.reverse_col_lookup(col) for col in needed_cols)
        wdata = data.reindex(sorted(needed_cols), axis=1)
        if added_cols is not None:
            wdata = pd.concat([wdata, pd.DataFrame(added_cols)], axis=1)


        replacements = {**replacements, **self.replacements}
        wdata.replace(replacements, inplace=True)

        if self.renames is not None:
            wdata.rename(columns = self.renames, inplace=True)

        return wdata


# Cell

def _convert_scales(scale_obj):

    data = pd.DataFrame(scale_obj).sort_values('Max')

    #print(data)

    index = pd.IntervalIndex.from_arrays(data['Min'].values, data['Max'].values,
                                         closed='both')
    #print(pd.Series(data['scaled'].values, index = index))

    return pd.Series(data['scaled'].values, index = index)

class ScaledRegressionNode(object):

    def __init__(self, field, scale_col, scales, flt, eqn, needed_cols,
                 result = 'zscale', transform = None):
        """

        Parameters
        ----------
        field : str
        scale_col : str
        scales : pd.Series
        flt : str
        eqn : str
        needed_cols : list[str]
        result : str
        replacements : dict
        """

        self.field = field
        self.scale_col = scale_col

        assert type(scales.index) == pd.IntervalIndex
        self.scales = scales

        self.flt = flt
        self.eqn = eqn
        self.needed_cols = needed_cols
        self.transformer = transform
        self.result = result

    @staticmethod
    def from_yaml(path, typ = 'tscore', transform = None):

        if transform is None:
            rev_lookup = lambda x: x
        else:
            rev_lookup = lambda x: transform.reverse_col_lookup(x)

        with open(path) as handle:
            obj = yaml.full_load(handle)
        try:
            return ScaledRegressionNode(field = obj['short_name'],
                                        scale_col = obj['scale_column'],
                                        scales = _convert_scales(obj['scores']),
                                        flt = obj['Population_filter'],
                                        eqn = obj[typ],
                                        needed_cols = [rev_lookup(x) for x in (obj['filter_cols'] + obj[typ+'_cols'])],
                                        result = typ,
                                        transform = transform)
        except ValueError:
            print(f'Failed on {path}')
            raise



    def extract_data(self, data, **kwargs):

        return self.transformer.extract_data(data, self.needed_cols, **kwargs)

    def extract_filter_data(self, data, **kwargs):

        return self.extract_data(data, **kwargs).query(self.flt)

    def scale_data(self, raw_values):
        """

        Parameters
        ----------
        raw_values : pd.Series

        Returns
        -------
        pd.Series

        """

        return raw_values.map(self.scales.get)

    def transform(self, data):
        return self.norm_data(data)

    def norm_data(self, data):

        scaled = self.scale_data(data[self.transformer.reverse_col_lookup(self.scale_col)])
        wdata = self.extract_filter_data(data, added_cols={'scaled': scaled})
        try:
            res = ne.evaluate(self.eqn, local_dict=wdata)
        except:
            print(wdata)
            raise

        res = pd.Series(res, index = wdata.index,
                        name = self.field)
        if self.result == 'tscore':
           res = (res-50)/10
        return res

# Cell

class ScaledRegressionTransform(object):

    def __init__(self, name = '', norm_nodes=None):
        """

        Parameters
        ----------
        norm_nodes : list[ScaledRegressionNode]
          Formatted lookup-table for the transformer
        """


        self.norm_nodes = norm_nodes
        self.name = name


    @staticmethod
    def from_yaml(paths, name = '', transform=None):
        nodes = [ScaledRegressionNode.from_yaml(path, transform=transform) for path in paths]
        return ScaledRegressionTransform(name = name,
                                         norm_nodes = nodes)

    def fit(self, *args, **kwargs):
        return self

    @property
    def needed_cols(self):
        cols = set()
        for node in self.norm_nodes:
            cols |= set(node.needed_cols)
        return sorted(cols)

    @property
    def out_fields(self):
        return sorted(set(node.field for node in self.norm_nodes))

    def get_feature_names(self):
        if len(self.name):
            return [self.name + '__' + node.field for node in self.norm_nodes]
        else:
            return self.out_fields

    def normalize_samples(self, data, aggfunc = 'last'):

        index_key = data.index.name
        if index_key is None:
            data.index.name = 'index'
            index_key = 'index'
        #print('index', index_key)
        out_data = []
        for node in self.norm_nodes:
            out_data.append(node.transform(data).reset_index())

        out_data = pd.concat(out_data, axis=0, ignore_index=True)
        if len(out_data.index) == 0: # nobody matched the filter
            return pd.Series([np.nan]*len(data.index),
                             index = data.index)

        out_piv = out_data.groupby(index_key)[self.out_fields].agg(aggfunc)
        return out_piv.reindex(data.index, axis=0)

    def transform(self, data):
        return self.normalize_samples(data, aggfunc = 'first')
