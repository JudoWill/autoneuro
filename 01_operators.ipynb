{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# default_exp operators\n",
    "\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numexpr as ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Operators\n",
    "\n",
    "In order to be able to capture the transformations required to normalize neuro data with different styles\n",
    "we'll need a collection of operations to manage them.\n",
    "In the abstract, these operations should manage grabbing the relevant data and then serving the result back in a standardized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "class AbstractOperation(object):\n",
    "\n",
    "    fields = []\n",
    "    result_fields = []\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        op_classes = [EquationOp,\n",
    "                      AggregationOp,\n",
    "                      ClipOp,\n",
    "                      NormativeLookupOp,\n",
    "                      CategoricalOp,\n",
    "                      BinnedScalingOp,\n",
    "                      EquationFilterOp]\n",
    "\n",
    "        for op_class in op_classes:\n",
    "            op = op_class.from_config(config)\n",
    "            if op is not None:\n",
    "                return op\n",
    "        raise NotImplementedError(f'Did not understand type: {config[\"type\"]}')\n",
    "        #return None\n",
    "\n",
    "    def process_single(self, row):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def explain(self, row):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def to_config(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def to_series(self, row):\n",
    "\n",
    "        series = pd.Series(dict((field, row.get(field)) for field in self.fields))\n",
    "        return series\n",
    "\n",
    "    def __call__(self, row):\n",
    "\n",
    "        res = self.process_single(row)\n",
    "        yield self.result_fields[0], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "abs_op = AbstractOperation()\n",
    "assert type(abs_op) == AbstractOperation\n",
    "\n",
    "abs_op.fields = ['t1', 't2']\n",
    "test_data = {'t1': 1, 't2': 2, 'other': 3}\n",
    "ser = abs_op.to_series(test_data)\n",
    "assert (ser['t1'] == 1) & (ser['t2'] == 2) & ('other' not in ser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll uss the BVMT test as the example.\n",
    "But we're going to back up a step. Since there are a bunch of different intermediate values, I want to calculate those using operations.\n",
    "\n",
    "Measured Values:\n",
    " - `trial1` - Trial 1 successes\n",
    " - `trial2` - Trial 2 successes\n",
    " - `trial3` - Trial 3 successes\n",
    " - `delay` - Delayed Successes\n",
    " - `hits` - Successful recognitions with distractors\n",
    " - `false_pos` - False-positive recognitions\n",
    "\n",
    "Our goal is to define all of the operations required to calculated intermediate values (ie immediate)\n",
    "as well as scaled values.\n",
    "\n",
    "There are three derived values to calculate:\n",
    "  - `immediate`: the sum of the three trials\n",
    "  - `regonition`: the number of hits - false-positive recognitions\n",
    "  - `retention`: ratio of delayed successes and largest of the trial 2 & trial 3 successes\n",
    "\n",
    "The first two can be solved with basic equations.\n",
    "The third will require an additional strategy effort.\n",
    "\n",
    "## Basic Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class EquationOp(AbstractOperation):\n",
    "    \"Manipulate values with 1numexpr1 equations.\"\n",
    "\n",
    "    def __init__(self, out_field, equation, fields):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        out_field : str\n",
    "        equation : str\n",
    "        fields : list[str]\n",
    "        \"\"\"\n",
    "\n",
    "        self.fields = fields\n",
    "        self.equation = equation\n",
    "        self.result_fields = [out_field]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        \"\"\"\n",
    "\n",
    "        Expecting yaml of the format:\n",
    "          type: equation\n",
    "          equation: \"hits-false_pos\"\n",
    "          fields: ['hits', 'false_pos']\n",
    "          out_field: 'recognition'\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        EquationOp\n",
    "        \"\"\"\n",
    "        if config['type'] == 'equation':\n",
    "            return EquationOp(config['out_field'],\n",
    "                              config['equation'],\n",
    "                              config['fields'])\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'equation',\n",
    "                'out_field': self.result_fields[0],\n",
    "                'equation': self.equation,\n",
    "                'fields': self.fields}\n",
    "    \n",
    "    \n",
    "\n",
    "    def explain(self, row):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row : dict,pd.Series\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "        \"\"\"\n",
    "\n",
    "        res = self.process_single(row)\n",
    "        return f'Used Equation: {self.equation} = {res} = {self.result_fields[0]}'\n",
    "\n",
    "    def process_single(self, row):\n",
    "        \"\"\" Apply the equation to the row\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row : mapping\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        #print(data)\n",
    "        if data.notnull().all():\n",
    "            res = pd.eval(self.equation, local_dict=data.to_dict())\n",
    "        else:\n",
    "            res = np.nan\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's imagine a individual to test.\n",
    "\n",
    "Measured Values:\n",
    " - `trial1` - 5\n",
    " - `trial2` - 6\n",
    " - `trial3` - 7\n",
    " - `delay` - 8\n",
    " - `hits` - 6\n",
    " - `false_pos` - 2\n",
    " - `copy` - 12\n",
    "\n",
    "Using the `EquationOp` let's calculate `immediate` and `recognition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA = {'bvmt_trial1': 5, 'bvmt_trial2': 6, 'bvmt_trial3': 7,\n",
    "        'bvmt_delay': 8, 'bvmt_hits': 6, 'bvmt_false_pos': 2,\n",
    "        'bvmt_copy': 12}\n",
    "\n",
    "total_op = EquationOp('bvmt_immediate',\n",
    "                      'bvmt_trial1+bvmt_trial2+bvmt_trial3',\n",
    "                      ['bvmt_trial1', 'bvmt_trial2', 'bvmt_trial3'])\n",
    "immed = total_op.process_single(DATA)\n",
    "assert immed == 18\n",
    "DATA['bvmt_immediate'] = immed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also `explain` the result using the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Equation: bvmt_trial1+bvmt_trial2+bvmt_trial3 = 18 = bvmt_immediate\n"
     ]
    }
   ],
   "source": [
    "print(total_op.explain(DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While one might construct these operations in Python code, I actually expect most things to be saved as yaml.\n",
    "So, we need a way to represent this info in that format.\n",
    "This is also useful when constructing larger sets.\n",
    "\n",
    "Here's the yaml example for the recognition calculation.\n",
    "```\n",
    "type: equation\n",
    "equation: \"hits-false_pos\"\n",
    "fields: ['hits', 'false_pos']\n",
    "out_field: 'recognition'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Equation: bvmt_hits-bvmt_false_pos = 4 = bvmt_recognition\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "st = \"\"\"\n",
    "type: equation\n",
    "equation: \"bvmt_hits-bvmt_false_pos\"\n",
    "fields: ['bvmt_hits', 'bvmt_false_pos']\n",
    "out_field: 'bvmt_recognition'\n",
    "\"\"\"\n",
    "\n",
    "ret_op = EquationOp.from_config(yaml.full_load(st))\n",
    "recog = ret_op.process_single(DATA)\n",
    "\n",
    "assert recog == 4\n",
    "print(ret_op.explain(DATA))\n",
    "DATA['bvmt_recognition'] = recog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Operations\n",
    "\n",
    "Due to limitations in numexpr, it cannot choose the largest of two numbers, as needed for retention.\n",
    "So, we use an `AggregationOp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class AggregationOp(AbstractOperation):\n",
    "\n",
    "    def __init__(self, out_field, aggregation, fields):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        out_field : str\n",
    "        aggregation : str\n",
    "        fields : list[str]\n",
    "        \"\"\"\n",
    "\n",
    "        self.fields = fields\n",
    "        self.aggregation = aggregation\n",
    "        self.result_fields = [out_field]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        \"\"\"\n",
    "        Load from config. Expects:\n",
    "            type: agg\n",
    "            method: 'max'\n",
    "            fields: ['trial2', 'trial3']\n",
    "            out_field: retention_denom\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        AggregationOp\n",
    "        \"\"\"\n",
    "        if config['type'] == 'agg':\n",
    "            return AggregationOp(config['out_field'],\n",
    "                                 config['method'],\n",
    "                                 config['fields'])\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'agg',\n",
    "                'out_field': self.result_fields[0],\n",
    "                'method': self.aggregation,\n",
    "                'fields': self.fields}\n",
    "    \n",
    "\n",
    "    def explain(self, row):\n",
    "        res = self.process_single(row)\n",
    "        return f'Aggregation: {self.aggregation} [{\", \".join(self.fields)}]  = {res}'\n",
    "\n",
    "    def process_single(self, row):\n",
    "        data = self.to_series(row)\n",
    "        return data.agg(self.aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation: max [bvmt_trial2, bvmt_trial3]  = 7\n"
     ]
    }
   ],
   "source": [
    "ret_denom_op = AggregationOp('bvmt_retention_denom', 'max', ['bvmt_trial2', 'bvmt_trial3'])\n",
    "re_denom = ret_denom_op.process_single(DATA)\n",
    "\n",
    "assert re_denom == 7\n",
    "print(ret_denom_op.explain(DATA))\n",
    "DATA['bvmt_retention_denom'] = re_denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have the denominator we can do another equation to calculate recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Equation: bvmt_delay/bvmt_retention_denom = 1.1428571428571428 = bvmt_retention\n"
     ]
    }
   ],
   "source": [
    "retent_op = EquationOp('bvmt_retention', 'bvmt_delay/bvmt_retention_denom', ['bvmt_delay', 'bvmt_retention_denom'])\n",
    "retent = retent_op.process_single(DATA)\n",
    "\n",
    "assert retent == 8/7\n",
    "print(retent_op.explain(DATA))\n",
    "DATA['bvmt_retention'] = retent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dang, the value was above 100%. And sometimes it may be negative.\n",
    "By convention we clip these to a [0,1] scale, which we'll need an operation for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class ClipOp(AbstractOperation):\n",
    "\n",
    "    def __init__(self, field, lower = 0, upper=1):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        field : str\n",
    "        lower : float\n",
    "        upper : float\n",
    "        \"\"\"\n",
    "\n",
    "        self.fields = [field]\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.result_fields = [field]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        \"\"\"\n",
    "        Load from config. Expects:\n",
    "            type: clip\n",
    "            field: retention\n",
    "            lower: 0\n",
    "            upper: 1\n",
    "        Parameters\n",
    "        ----------\n",
    "        config\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ClipOp\n",
    "\n",
    "        \"\"\"\n",
    "        if config['type'] == 'clip':\n",
    "            return ClipOp(config['field'],\n",
    "                          lower = config['lower'],\n",
    "                          upper = config['upper'])\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'clip',\n",
    "                'lower': self.lower,\n",
    "                'upper': self.aggregation,\n",
    "                'field': self.fields[0]}\n",
    "\n",
    "    def explain(self, row):\n",
    "        return f'Clipped {self.fields[0]} to [{self.lower}, {self.upper}]'\n",
    "\n",
    "    def process_single(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        clipped = data.clip(lower=self.lower, upper=self.upper)\n",
    "        return clipped[self.result_fields[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped bvmt_retention to [0, 1]\n"
     ]
    }
   ],
   "source": [
    "ret_clip_op = ClipOp('bvmt_retention', lower=0, upper=1)\n",
    "ret_clip = ret_clip_op.process_single(DATA)\n",
    "\n",
    "assert ret_clip == 1\n",
    "print(ret_clip_op.explain(DATA))\n",
    "DATA['bvmt_retention'] = ret_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we've calculated all of the intermediate values.\n",
    "We'll need to use these values, along with demographic data, to lookup a \"healthy\" normal distribution.\n",
    "\n",
    "## Normative Lookups\n",
    "\n",
    "These lookup tables are composed of lookup tables that index a mean and std given a demographic filter.\n",
    "For example:\n",
    "\n",
    "A 32 year old should have an `immediate` memory of 26.9 with a std of 4.6.\n",
    "Our example has an `immediate` of 18, a deficit of 9, roughly 2 stds.\n",
    "\n",
    "We'll need to make an object to contain the logic of matching and filtering each \"element\" of the table. This will be useful to deal with many different types of normative lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "\n",
    "\n",
    "class AbstractNormative(object):\n",
    "    \n",
    "    flt = None\n",
    "    \n",
    "    def to_config(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        for cl in [MeanStdNormative, LookupNormative]:\n",
    "            obj = cl.from_config(config)\n",
    "            if obj is not None:\n",
    "                return obj\n",
    "        raise ValueError(f'Could not understand config: {config}')\n",
    "        \n",
    "    \n",
    "    def explain(self, value): raise NotImplementedError\n",
    "    \n",
    "    def scale(self, data): raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    def is_valid(self, data):\n",
    "        return ne.evaluate(self.flt, local_dict=data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `MeanStdNormative` represents a table where a mean/std is provided for healthy individuals for a given filter criteria. Each element of the table is one `MeanStdNormative` object and the `NormativeLookupOp` manages a list of these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MeanStdNormative(AbstractNormative):\n",
    "    \"\"\"Deal with mean/std scaled -> percentile lookup tables\"\"\"\n",
    "    \n",
    "    def __init__(self, flt, mean, std):\n",
    "        self.flt = flt\n",
    "        self.mean, self.std = mean, std\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        if ('mean' in config) and ('std' in config):\n",
    "            return MeanStdNormative(config['filter'], \n",
    "                                    config['mean'], \n",
    "                                    config['std'])\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'mean_std', \n",
    "                'filter': self.filter, \n",
    "                'mean': self.mean, 'std': self.std}\n",
    "    \n",
    "    def scale(self, value):\n",
    "        return (value-self.mean)/self.std\n",
    "    \n",
    "    def explain(self, value):\n",
    "        \n",
    "        scaled = self.scale(value)\n",
    "        matched = f'Matched: {self.flt}'\n",
    "        calculated = f'Expected {self.mean}+/-{self.std} but observed {value}'\n",
    "        result = f'Scaled to: z={scaled}'\n",
    "        return '\\n'.join([matched, calculated, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class NormativeLookupOp(AbstractOperation):\n",
    "    \"\"\"Lookup table with normalized scores.\"\"\"\n",
    "\n",
    "    def __init__(self, lookup_table, filter_cols, measure_col, out_name):\n",
    "\n",
    "        self.lookup_table = lookup_table\n",
    "        self.filter_cols = filter_cols\n",
    "        self.fields = filter_cols + [measure_col]\n",
    "        self.result_fields = [out_name]\n",
    "        self.measure_col = measure_col\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "\n",
    "        if config['type'] == 'normative_lookup':\n",
    "\n",
    "            return NormativeLookupOp([AbstractNormative.from_config(row) for row in config['table']],\n",
    "                                     config['filter_cols'],\n",
    "                                     config['measure_col'],\n",
    "                                     config['out_name'])\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'normative_lookup',\n",
    "                'table': [norm.to_config() for norm in self.lookup_table],\n",
    "                'filter_cols': self.filter_cols,\n",
    "                'measure_col': self.measure_col,\n",
    "                'out_name': self.result_fields[0]}\n",
    "\n",
    "    def lookup_norm(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        for norm in self.lookup_table:\n",
    "            if norm.is_valid(data):\n",
    "                return norm\n",
    "\n",
    "        return None\n",
    "\n",
    "    def explain(self, row):\n",
    "\n",
    "        norm = self.lookup_norm(row)\n",
    "        data = self.to_series(row)\n",
    "\n",
    "        if norm is None:\n",
    "            return f'{self.result_fields[0]}: Could not find matching filter for {data[self.filter_cols]}'\n",
    "        else:\n",
    "            return norm.explain(data[self.measure_col])\n",
    "\n",
    "    def process_single(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        norm = self.lookup_norm(data)\n",
    "        if norm is None:\n",
    "            return np.nan\n",
    "        r = norm.scale(data[self.measure_col])\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While it is possible to create these in Python, it's much easier to build using yaml definitions.\n",
    "\n",
    "```\n",
    "type: normative_lookup\n",
    "measure_col: immediate\n",
    "filter_cols: ['age']\n",
    "out_name: 'heaton_immediate'\n",
    "table:\n",
    "  - filter: (18 <= age) & (age <= 21)\n",
    "    mean: 28.74\n",
    "    std: 4.32\n",
    "  - filter: (20 <= age) & (age <= 23)\n",
    "    mean: 28.44\n",
    "    std: 4.38\n",
    "  ...\n",
    "\n",
    "```\n",
    "\n",
    "The filters are anything acceptable to `pd.eval`.\n",
    "The Heaton norms for the BVMT are currently in `data/norms/from_kate/heaton_bvmt.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: (30 <= age) & (age <= 33)\n",
      "Expected 26.92+/-4.64 but observed 18\n",
      "Scaled to: z=-1.9224137931034488\n"
     ]
    }
   ],
   "source": [
    "DATA['age'] = 32\n",
    "bvmt_config = yaml.full_load(open('data/norms/from_kate/heaton_bvmt.yaml'))\n",
    "\n",
    "lookup_op = NormativeLookupOp.from_config(bvmt_config['operations'][0])\n",
    "lookup_score = lookup_op.process_single(DATA)\n",
    "\n",
    "assert lookup_score == -1.9224137931034488\n",
    "print(lookup_op.explain(DATA))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The collection of these operators can be combined into a `TestCalculator` which manages applying these operations sequentially.\n",
    "That is discussed elsewhere.\n",
    "\n",
    "These features are sufficient for any analysis that requires looking up normalizations based on demographic information.\n",
    "However, for regression based norms like the `Norman` set we need a further collection of operators.\n",
    "\n",
    "## Regression Based Norms\n",
    "\n",
    "When doing regression based normalization the first step is to `scale` the raw values based on a set of bins.\n",
    "This is done to help _normalize_ the raw values before entering the regression equation.\n",
    "\n",
    "For example. When scaling the `delay` column, the `norman` scheme uses:\n",
    "\n",
    "| Raw | Scaled |\n",
    "|-----|--------|\n",
    "| 12  | 14\n",
    "| 11  | 11\n",
    "| 10  | 9\n",
    "| 9   | 8\n",
    "| 8   | 7\n",
    "| 7   | 6\n",
    "| 5   | 5\n",
    "| 4   | 4\n",
    "| 3   | 3\n",
    "| 0   | 2\n",
    "\n",
    "The `BinnedScalingOp` can be used to deal with these conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BinnedScalingOp(AbstractOperation):\n",
    "    def __init__(self, bins, measure_col, out_field = None):\n",
    "\n",
    "\n",
    "        self.fields = [measure_col]\n",
    "        if out_field is None:\n",
    "            self.result_fields = [measure_col+'_scaled']\n",
    "        else:\n",
    "            self.result_fields = [out_field]\n",
    "        self.bins = sorted(bins, key = lambda x: x['min'],\n",
    "                           reverse=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        \"\"\"\n",
    "        Build from config, Expecting yaml like:\n",
    "          type: binned_scaling\n",
    "          measure_col: delay\n",
    "          bins:\n",
    "            - scaled: 14\n",
    "              min: 12\n",
    "            - scaled: 11\n",
    "              min: 11\n",
    "            - scaled: 9\n",
    "              min: 10\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        BinnedScalingOp\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if config['type'] == 'binned_scaling':\n",
    "            return BinnedScalingOp(config['bins'],\n",
    "                                   config['measure_col'])\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'binned_scaling',\n",
    "                'bins': self.bins,\n",
    "                'measure_col': self.measure_col}\n",
    "\n",
    "    def lookup_bin(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        val = data[self.fields[0]]\n",
    "        if val == val:\n",
    "            for bin in self.bins:\n",
    "                if val >= bin['min']:\n",
    "                    return bin['min'], bin['scaled']\n",
    "            return np.nan, np.nan\n",
    "        else:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "    def explain(self, row):\n",
    "\n",
    "        edge, scaled = self.lookup_bin(row)\n",
    "\n",
    "        if edge != edge:\n",
    "            data = self.to_series(row)\n",
    "            return f'Could not find matching bin for {data[self.fields[0]]}'\n",
    "        else:\n",
    "            return f'{self.fields[0]} matched {edge}, scaled to {scaled}'\n",
    "\n",
    "    def process_single(self, row):\n",
    "        _, res = self.lookup_bin(row)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bvmt_delay matched 8, scaled to 7\n"
     ]
    }
   ],
   "source": [
    "bins = [{'min': 12, 'scaled': 14},\n",
    "        {'min': 11, 'scaled': 11},\n",
    "        {'min': 10, 'scaled': 9},\n",
    "        {'min': 9, 'scaled':  8},\n",
    "        {'min': 8, 'scaled': 7},\n",
    "        {'min': 7, 'scaled': 6},\n",
    "        {'min': 5, 'scaled': 5},\n",
    "        {'min': 4, 'scaled': 4},\n",
    "        {'min': 3, 'scaled': 3},\n",
    "        {'min': 0, 'scaled': 2}]\n",
    "\n",
    "scale_op = BinnedScalingOp(bins, 'bvmt_delay')\n",
    "delay_scaled = scale_op.process_single(DATA)\n",
    "\n",
    "assert delay_scaled == 7\n",
    "print(scale_op.explain(DATA))\n",
    "\n",
    "DATA['bvmt_delay_scaled'] = delay_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have the scaled value we need to handle the demographic variables.\n",
    "For the `norman` set Male gender is set to 1 with females as 0.\n",
    "The race also needs to be converted with white = 0 and AA = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CategoricalOp(AbstractOperation):\n",
    "\n",
    "    def __init__(self, measure_col, mapping, out_col):\n",
    "\n",
    "        self.result_fields = [out_col]\n",
    "        self.mapping = mapping\n",
    "        self.fields = [measure_col]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        \"\"\"\n",
    "        Build from config. Expects yaml like:\n",
    "          type: categorical\n",
    "          in_field: gender\n",
    "          out_field: norman_gender\n",
    "          mapping:\n",
    "            male: 0\n",
    "            female: 1\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if config['type'] == 'categorical':\n",
    "            return CategoricalOp(config['in_field'],\n",
    "                                 config['mapping'],\n",
    "                                 config['out_field'])\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'categorical',\n",
    "                'in_field': self.fields[0],\n",
    "                'mapping': self.mapping,\n",
    "                'out_field': self.result_fields[0]}\n",
    "\n",
    "    def lookup(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        return self.mapping.get(data[self.fields[0]])\n",
    "\n",
    "    def process_single(self, row):\n",
    "\n",
    "        return self.lookup(row)\n",
    "\n",
    "    def explain(self, row):\n",
    "\n",
    "        res = self.lookup(row)\n",
    "        if res is not None:\n",
    "            return f'{self.fields[0]}:{row[self.fields[0]]} -> {self.result_fields[0]}:{res}'\n",
    "        else:\n",
    "            return f'Could not match {self.fields[0]}:{row[self.fields[0]]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender:male -> norman_gender:0\n"
     ]
    }
   ],
   "source": [
    "cat_op = CategoricalOp('gender', {'male': 0, 'female': 1}, 'norman_gender')\n",
    "DATA['gender'] = 'male'\n",
    "\n",
    "norman_gender = cat_op.process_single(DATA)\n",
    "\n",
    "assert norman_gender == 0\n",
    "print(cat_op.explain(DATA))\n",
    "DATA['norman_gender'] = norman_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now for the big finale, regression based norms.\n",
    "After scaling the relevant data and handling categorical variables we need to apply an equation.\n",
    "However, the equation changes depending on the individual's demographic variables.\n",
    "One for african americans, one for caucasians, and a different one for spanish speakers.\n",
    "The `EquationFilterOp` takes care of these intricacies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class EquationFilterOp(AbstractOperation):\n",
    "\n",
    "    def __init__(self, fields, regressions, out_field, result_type = 'zscale'):\n",
    "\n",
    "        self.regressions = regressions\n",
    "        self.fields = fields\n",
    "        self.result_fields = [out_field]\n",
    "        self.result_type = result_type\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        if config['type'] == 'equation_filter':\n",
    "            return EquationFilterOp(config['fields'],\n",
    "                                    config['equations'],\n",
    "                                    config['out_field'],\n",
    "                                    result_type = config['result_type'])\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'equation_filter',\n",
    "                'fields': self.fields,\n",
    "                'equations': self.regressions,\n",
    "                'result_type': self.result_type,\n",
    "                'out_field': self.result_fields[0]}\n",
    "\n",
    "    def search_filters(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        check_func = lambda reg: pd.eval(reg['filter'], local_dict=data.to_dict())\n",
    "        return [reg for reg in self.regressions if check_func(reg)]\n",
    "\n",
    "    def scale_data(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        hits = self.search_filters(row)\n",
    "        if hits: #Currently only implementing \"first\"\n",
    "            reg = hits[0]\n",
    "            val = pd.eval(reg['norm'], local_dict=data.to_dict())\n",
    "\n",
    "            if (self.result_type == 'standard_score') | (self.result_type == 'tscore'):\n",
    "                val = (val - 50)/10\n",
    "            elif (self.result_type  == 'zscore') | (self.result_type  == 'zscale'):\n",
    "                pass\n",
    "            elif self.result_type == 'other':\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f'Did not understand result_type: {self.result_type}')\n",
    "\n",
    "            return reg, val\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    def explain(self, row):\n",
    "\n",
    "        data = self.to_series(row)\n",
    "        reg, val = self.scale_data(row)\n",
    "\n",
    "        if reg is None:\n",
    "            return 'Could not find a match for regression normalization.'\n",
    "        else:\n",
    "            return f'Matched {reg[\"filter\"]}, applied {reg[\"norm\"]} = {float(val)}'\n",
    "\n",
    "\n",
    "    def process_single(self, row):\n",
    "\n",
    "        _, val = self.scale_data(row)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These are best explained through their yaml imports.\n",
    "Examine the `data/norms/norman/norman_bvmt_regnorm.yaml` for a complete example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidemensional Lookups\n",
    "\n",
    "Some set of Heaton Norms (Grooved Peg, trails, etc) require a multi-step lookup. The original tables look like this:\n",
    "\n",
    "| 5 | 4 | 3 | 2 | 1 | Education | Gender | Age\n",
    "|---|---|---|---|---|-----------|--------|----\n",
    "| 100 | 80 | 70 | 50 | 10 | 0 | 0 | 0\n",
    "| 100 | 85 | 72 | 48 | 11 | 0 | 1 | 0\n",
    "| 100 | 82 | 75 | 42 | 15 | 0 | 0 | 1\n",
    "| 100 | 82 | 75 | 42 | 15 | 0 | 1 | 1\n",
    "\n",
    "Where the first columns refer to Scaled scores and the last columns refer to categorical features. In this case, the intent would be to get the appropriate scaled score for the test, look to the row that corresponds with the patient's Education, Gender, and Age.\n",
    "\n",
    "These are actually just more cases of `NormativeLookupOp` and `MeanStdNormative`. So, the approach is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general `LookupNormative` class takes a mapping dictionary that maps raw values to scaled values. These can then be post-processed using `post`. This can convert _standard scores_ and _percentiles_ to _z-scales_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import scipy.stats as st\n",
    "\n",
    "class LookupNormative(AbstractNormative):\n",
    "    \"\"\"Deal with scaled -> percentile lookup tables\"\"\"\n",
    "    \n",
    "    def __init__(self, flt, mapping, post = None):\n",
    "        self.flt = flt\n",
    "        self.mapping = mapping # A dict {'raw': 'scaled', 'raw': 'scaled'}\n",
    "        self.post = post\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        if ('mapping' in config):\n",
    "            return LookupNormative(config['filter'], \n",
    "                                   config['mapping'], \n",
    "                                   post = config.get('post', None))\n",
    "        return None\n",
    "    \n",
    "    def to_config(self):\n",
    "        \n",
    "        return {'type': 'lookup', \n",
    "                'filter': self.filter, \n",
    "                'mapping': self.mapping,\n",
    "                'post': self.post}\n",
    "    \n",
    "    def scale(self, value):\n",
    "        try:\n",
    "            scaled = self.mapping[value]\n",
    "        except KeyError:\n",
    "            return np.nan\n",
    "        \n",
    "        if self.post == 'leave':\n",
    "            pass\n",
    "        elif self.post == 'ss2z':\n",
    "            # standard (50/10) to Z (0/1)\n",
    "            scaled = (scaled - 50)/10\n",
    "        elif self.post == 'percentile2z':\n",
    "            scaled = st.norm.ppf(scaled/100)        \n",
    "            \n",
    "        return scaled\n",
    "        \n",
    "    def explain(self, value):\n",
    "        \n",
    "        try:\n",
    "            mapped = self.mapping[value]\n",
    "        except KeyError:\n",
    "            mapped = 'missing'\n",
    "        scaled = self.scale(value)\n",
    "        \n",
    "        matched = f'Matched: {self.flt}'\n",
    "        calculated = f'Mapped to: {mapped}'\n",
    "        if (self.post is None) or (self.post == 'leave'):\n",
    "            result = ''\n",
    "        else: result = f'Scaled to: z={scaled}'\n",
    "            \n",
    "        return '\\n'.join([matched, calculated, result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `MultiLookupOp` is just a light wrapper around the `NormativeLookupOp` that implements some helper features to create these items from \"Kate Excel Sheet\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MultiLookupOp(NormativeLookupOp):\n",
    "    \n",
    "    def __init__(self, lookup_table, filter_cols, measure_col, out_name):\n",
    "\n",
    "        self.lookup_table = lookup_table\n",
    "        self.filter_cols = filter_cols\n",
    "        self.fields = filter_cols + [measure_col]\n",
    "        self.result_fields = [out_name]\n",
    "        self.measure_col = measure_col\n",
    "    \n",
    "    def incoperate(self, other):\n",
    "        \n",
    "        self.lookup_table += other.lookup_table\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def from_sheet_format(path, filter_mappings, filter_cols,\n",
    "                          measure_col, out_name, post = 'ss2z', extra_filter = None):\n",
    "        \n",
    "        data = pd.read_csv(path)\n",
    "        scale_cols = [col for col in data.columns if col not in filter_mappings]\n",
    "        \n",
    "        lookup_table = []\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            flt = []\n",
    "            for col, mapping in filter_mappings.items():\n",
    "                flt.append(mapping[row[col]])\n",
    "            flt = ' & '.join(f'({fl})' for fl in flt)\n",
    "            \n",
    "            if extra_filter is not None:\n",
    "                flt = f'({extra_filter}) & ({flt})'\n",
    "                                    \n",
    "            lookup_table.append(LookupNormative(flt,\n",
    "                                                dict((int(sc), row[sc]) for sc in scale_cols),\n",
    "                                                post = post))\n",
    "        \n",
    "        return MultiLookupOp(lookup_table, filter_cols,\n",
    "                             measure_col, out_name)\n",
    "                            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "\n",
    "HEATON_MAPPINGS = {'Age':{1: '(0 < age) & (age < 35)',\n",
    "                          2: '(35 <= age) & (age < 40)',\n",
    "                          3: '(40 <= age) & (age < 45)',\n",
    "                          4: '(45 <= age) & (age < 50)',\n",
    "                          5: 'age >= 50'},\n",
    "                   'Education':{1: '(0 < education) & (education <= 9)',\n",
    "                                2: '(9 < education) & (education <= 12)',\n",
    "                                3: '(12 < education) & (education <= 13)',\n",
    "                                4: '(13 < education) & (education <= 16)',\n",
    "                                5: '(16 < education) & (education <= 18)',\n",
    "                                6: '(education > 18)',\n",
    "                               },\n",
    "                   'Gender': {1: 'heaton_gender == 1',\n",
    "                              2: 'heaton_gender == 2'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_op = MultiLookupOp.from_sheet_format('data/norms/from_kate/sheets/GPD.csv',\n",
    "                                           HEATON_MAPPINGS,\n",
    "                                           ['age', 'heaton_gender', 'education'],\n",
    "                                           'grooved_peg_dom_scaled',\n",
    "                                           'grooved_peg_dom_heaton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: ((45 <= age) & (age < 50)) & ((12 < education) & (education <= 13)) & (heaton_gender == 1)\n",
      "Mapped to: 57\n",
      "Scaled to: z=0.7\n"
     ]
    }
   ],
   "source": [
    "print(multi_op.explain({'age': 45, 'heaton_gender': 1, 'education': 13,\n",
    "                        'grooved_peg_dom_scaled': 11}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
